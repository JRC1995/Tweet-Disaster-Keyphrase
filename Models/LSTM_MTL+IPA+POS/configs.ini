[HYPERPARAMETERS]
batch_size = 32
window_size = 1
model = BIBILSTM
; choose between BIULSTM (BI_LSTM-LSTM stack), BIBILSTM (BI_LSTM-BI_LSTM stack), and TRANSFORMER (not yet implemented, wouldn't work actually right now)
hidden_size = 300
alpha = 0.5
; should be 0 to 1
grad_max_norm = 5
learning_rate = 0.0015
momentum = 0.9
decay = 0.05
fine_tune_lr = 1e-4
fine_tune = True
gross_tune = False
; if True embeddings are trained at the same rate as others and from the start
l2 = 1e-6
dropout = 0.5
epochs = 100
freeze_epochs = 2
variational_dropout = False
optimizer_to_use = nadam
embedding_dropout = False
